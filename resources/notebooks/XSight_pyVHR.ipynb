{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/androbaza/Xsight-heart-rate/blob/main/resources/notebooks/XSight_pyVHR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RndCVqh915_g"
      },
      "source": [
        "<a id='setup'></a>\n",
        "#Initial setup \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/env-check.py"
      ],
      "metadata": {
        "id": "6qqE2tWpSFDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will update the Colab environment and restart the kernel.  Don't run the next cell until you see the session crash.\n",
        "!bash rapidsai-csp-utils/colab/update_gcc.sh\n",
        "import os\n",
        "os._exit(00)"
      ],
      "metadata": {
        "id": "2MDapIyASIeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will install CondaColab.  This will restart your kernel one last time.  Run this cell by itself and only run the next cell once you see the session crash.\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "id": "7FpCw7U8SLFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you can now run the rest of the cells as normal\n",
        "import condacolab\n",
        "condacolab.check()"
      ],
      "metadata": {
        "id": "TtnklollSNUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing RAPIDS is now 'python rapidsai-csp-utils/colab/install_rapids.py <release> <packages>'\n",
        "# The <release> options are 'stable' and 'nightly'.  Leaving it blank or adding any other words will default to stable.\n",
        "!python rapidsai-csp-utils/colab/install_rapids.py stable\n",
        "import os\n",
        "os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n",
        "os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n",
        "os.environ['CONDA_PREFIX'] = '/usr/local'"
      ],
      "metadata": {
        "id": "jHwizO9aSPja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "id": "f587aTXxRT0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall pyvhr"
      ],
      "metadata": {
        "id": "9o_DjK2AtJgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/phuselab/pyVHR\n",
        "%cd ./pyVHR\n",
        "!python setup.py install"
      ],
      "metadata": {
        "id": "9K3tsFwJcbKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe setuptools lmfit biosppy scikit_posthocs autorank #cusignal"
      ],
      "metadata": {
        "id": "ardWx91bdYRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvhr"
      ],
      "metadata": {
        "id": "nGsBCnT9sdzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pyVHR\n",
        "from pyVHR.analysis.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pipe = Pipeline()\n",
        "time, BPM, uncertainty = pipe.run_on_video('/content/3.mp4', roi_approach=\"patches\", roi_method=\"faceparsing\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(time, BPM)\n",
        "plt.fill_between(time, BPM-uncertainty, BPM+uncertainty, alpha=0.2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8-vDfyv8P2gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnKKDdQ_ZhGO"
      },
      "source": [
        "# MTTS-CAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd9-878FZc88"
      },
      "source": [
        "**DeepPhys: Video-Based Physiological Measurement Using Convolutional Attention Networks**\n",
        "\n",
        "*Weixuan Chen and Daniel McDuff*\n",
        "\n",
        "**Abstract**. Non-contact video-based physiological measurement has many applications in health care and human-computer interaction. Practical applications require measurements to be accurate even in the presence of large head rotations. We propose the first end-to-end system for video-based measurement of heart and breathing rate using a deep convolutional network. The system features a new motion representation based on a skin reflection model and a new attention mechanism using appearance information to guide motion estimation, both of which enable robust measurement under heterogeneous lighting and major motions. Our approach significantly outperforms all current state-of-the-art methods on both RGB and infrared video datasets. Furthermore, it allows spatial-temporal distributions of physiological signals to be visualized via the attention mechanism.\n",
        "\n",
        "papers: \n",
        "* [DeepPhys: Video-Based Physiological Measurement Using Convolutional Attention Networks](https://web.media.mit.edu/~cvx/docs/18.Chen-etal-ECCV.pdf), \n",
        "* [Multi-Task Temporal Shift Attention Networks for On-Device Contactless Vitals Measurement\n",
        "](https://papers.nips.cc/paper/2020/file/e1228be46de6a0234ac22ded31417bc7-Paper.pdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2WR2MU6L-y1"
      },
      "outputs": [],
      "source": [
        "# extract raw frames\n",
        "sp = vhr.extraction.sig_processing.SignalProcessing()\n",
        "frames = sp.extract_raw(videoFileName)\n",
        "print('Frames shape:', frames.shape)\n",
        "\n",
        "# apply MTTS_CAN model\n",
        "bvp_pred = vhr.deepRPPG.MTTS_CAN_deep(frames, fps, verb=1)\n",
        "bvps = vhr.BPM.BVPsignal(bvp_pred, fps) # BVP object\n",
        "vhr.plot.visualize_BVPs([bvps.data], 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nVCmksezBGq"
      },
      "outputs": [],
      "source": [
        "## -- analysis\n",
        "from pyVHR.utils.errors import getErrors, printErrors, displayErrors, BVP_windowing\n",
        "from pyVHR.extraction.utils import sliding_straded_win_offline\n",
        "\n",
        "# BVP windowing & BPM estimate\n",
        "bvp_win, timesES = BVP_windowing(bvp_pred, winSizeGT, fps, stride=1)\n",
        "bpmES = vhr.BPM.BVP_to_BPM(bvp_win, fps) \n",
        "\n",
        "# compute and print errors\n",
        "RMSE, MAE, MAX, PCC, CCC, SNR = vhr.utils.getErrors(bvp_win, fps, bpmES, bpmGT, timesES, timesGT)\n",
        "vhr.utils.printErrors(RMSE, MAE, MAX, PCC, CCC, SNR)\n",
        "displayErrors(bpmES, bpmGT, timesES, timesGT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgMi3OkXdkIq"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcHFHqIj0BQ8"
      },
      "outputs": [],
      "source": [
        "# run on a single video\n",
        "from pyVHR.analysis.pipeline import DeepPipeline\n",
        "\n",
        "filename = '/var/datasets/VHR1/UBFC1/after-exercise/vid.avi'\n",
        "\n",
        "pipe = DeepPipeline()\n",
        "\n",
        "time, BPM = pipe.run_on_video(filename, method='MTTS_CAN')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "XSight-pyVHR.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}